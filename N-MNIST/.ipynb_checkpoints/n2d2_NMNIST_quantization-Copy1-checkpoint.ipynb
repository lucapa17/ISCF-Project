{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c0cbaa-e1f8-42de-bb65-e425966866de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import n2d2\n",
    "from n2d2.cells.nn import Fc, Conv, Pool2d\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c35364-9627-4b92-a0c3-7075208fb269",
   "metadata": {},
   "source": [
    "## N-MNIST - Final Table\n",
    "\n",
    "# 1st Architecure (2D - Channels)\n",
    "| Quantization Level | Accuracy | Training Time | Inference Time |\n",
    "| ------------------- | -------- | ------------- | -------------- |\n",
    "| No quantization     | 95.29%      | 53 min 53 sec    | 9.985432 sec    |\n",
    "| 8 bits              | 90.63%     | 43 min 18 sec    | 12.000157 sec    |\n",
    "| 4 bits              | 91.71%      | 47 min 13 sec    | 12.719034 sec     |\n",
    "| 2 bits              | 85.33%     | 45 min 23 sec     | 10.8017 sec   |\n",
    "| 1 bit               | 81.09%     | 43 min 57 sec    | 12.456814 sec    |\n",
    "\n",
    "# 2st Architecure (Deeper) (2D - Channels)\n",
    "| Quantization Level | Accuracy | Training Time | Inference Time |\n",
    "| ------------------- | -------- | ------------- | -------------- |\n",
    "| No quantization     | 95.81%      | 91 min 32 sec   | 23.22939 sec   |\n",
    "| 8 bits              | 96.99%    | 90 min 57 sec   | 27.172313 sec    |\n",
    "| 4 bits              | 95.82%    | 89 min 20 sec    | 28.383955 sec     |\n",
    "| 2 bits              | 94.49%     | 88 min 16 sec      | 28.245618 sec    |\n",
    "| 1 bit               | 93.31%    | 88 min 44 sec     | 26.153251 sec   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86c596f-d458-4f38-abd5-3a13f6e812b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_conf = n2d2.ConfigSection(\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "\n",
    "def conv_conf():\n",
    "    return n2d2.ConfigSection(\n",
    "        activation=n2d2.activation.Rectifier(),\n",
    "        no_bias=True,\n",
    "        weights_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        bias_solver=n2d2.solver.Adam(**solver_conf),\n",
    "    )\n",
    "    \n",
    "def fc_conf1():\n",
    "    return n2d2.ConfigSection(\n",
    "        activation=n2d2.activation.Rectifier(),\n",
    "        no_bias=True,\n",
    "        weights_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        bias_solver=n2d2.solver.Adam(**solver_conf),\n",
    "    )\n",
    "def fc_conf2():\n",
    "    return n2d2.ConfigSection(\n",
    "        activation=n2d2.activation.Linear(),\n",
    "        no_bias=True,\n",
    "        weights_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        bias_solver=n2d2.solver.Adam(**solver_conf),\n",
    "    )\n",
    "    \n",
    "# Definition of layers for quantization\n",
    "\n",
    "def conv_quantization_conf(n_bits):\n",
    "    return n2d2.ConfigSection(\n",
    "        activation=n2d2.activation.Rectifier(),\n",
    "        no_bias=True,\n",
    "        weights_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        bias_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        quantizer=n2d2.quantizer.SATCell(\n",
    "            apply_scaling=True,\n",
    "            apply_quantization=True,\n",
    "            range=2**n_bits-1,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def fc_quantization_conf1(n_bits):\n",
    "    return n2d2.ConfigSection(\n",
    "        activation=n2d2.activation.Rectifier(),\n",
    "        no_bias=True,\n",
    "        weights_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        bias_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        quantizer=n2d2.quantizer.SATCell(\n",
    "            apply_scaling=True,\n",
    "            apply_quantization=True,\n",
    "            range=2**n_bits-1,\n",
    "        ),\n",
    "    )\n",
    "def fc_quantization_conf2(n_bits):\n",
    "    return n2d2.ConfigSection(\n",
    "        activation=n2d2.activation.Linear(),\n",
    "        no_bias=True,\n",
    "        weights_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        bias_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        quantizer=n2d2.quantizer.SATCell(\n",
    "            apply_scaling=True,\n",
    "            apply_quantization=True,\n",
    "            range=2**n_bits-1,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc8fb1-8fc4-4dd2-bc07-df860c3be44f",
   "metadata": {},
   "source": [
    "## N-MNIST : 2 channels (positive events, negative events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b879fc01-1ff5-44ca-a456-7f8f9f9d15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_2_channels.pkl', 'rb') as file:\n",
    "    data_2_channels = pickle.load(file)\n",
    "with open('labels_2_channels.pkl', 'rb') as file:\n",
    "    labels_2_channels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23cd8ad2-3e5f-49a7-8958-9c6443f1ce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Create Provider ###\n",
      "'DataProvider_0' DataProvider(database=Numpy(composite_label=Auto, data_file_label=True, default_label=, multi_channel_match=, multi_channel_replace=, rois_margin=0, random_partitioning=False, target_data_path=), size=[28, 28, 2], batch_size=64)\n"
     ]
    }
   ],
   "source": [
    "db = n2d2.database.Numpy(random_partitioning=False)\n",
    "\n",
    "db.load(data_2_channels, labels_2_channels)\n",
    "db.partition_stimuli(5/7, 1/7, 1/7) # training: 50k, validation: 10k, test: 10k\n",
    "\n",
    "print(\"\\n### Create Provider ###\")\n",
    "batch_size = 64\n",
    "provider = n2d2.provider.DataProvider(db, [28, 28, 2], batch_size=batch_size)\n",
    "print(provider)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7843e6-0d6f-4749-8234-d4a447537e9e",
   "metadata": {},
   "source": [
    "# Model no quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4af09a1e-a723-469b-a31f-0d4c3f446f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Loading Model (without quantization) ###\n",
      "'Sequence_0' Sequence(\n",
      "\t(0): 'Conv_0' Conv(Frame<float>)(nb_inputs=2, nb_outputs=6, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc599ebc2f0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=None)\n",
      "\t(1): 'Pool2d_0' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(2): 'Conv_1' Conv(Frame<float>)(nb_inputs=6, nb_outputs=16, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc55f1b2370>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=None)\n",
      "\t(3): 'Pool2d_1' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(4): 'Conv_2' Conv(Frame<float>)(nb_inputs=16, nb_outputs=120, kernel_dims=[4, 4], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc55f1bd570>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=None)\n",
      "\t(5): 'Fc_0' Fc(Frame<float>)(nb_inputs=120, nb_outputs=84 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc55ca2ddb0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=None)\n",
      "\t(6): 'Fc_1' Fc(Frame<float>)(nb_inputs=84, nb_outputs=10 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Linear(clipping=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc5401f8e70>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### Loading Model (without quantization) ###\")\n",
    "model_2_channels = n2d2.cells.Sequence([\n",
    "    Conv(2, 6, kernel_dims=[5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [4, 4], **conv_conf()),\n",
    "    Fc(120, 84, **fc_conf1()),\n",
    "    Fc(84, 10, **fc_conf2()),\n",
    "])\n",
    "print(model_2_channels)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0ed69-2d29-4e43-be58-d1199c643e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_2_channels.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_2_channels(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_2_channels.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "        \n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220da472-ebbe-496e-b129-24716a903ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_2_channels/Conv_0.syntxt\n",
      "Import ./model_2_channels/Conv_1.syntxt\n",
      "Import ./model_2_channels/Conv_2.syntxt\n",
      "Import ./model_2_channels/Fc_0.syntxt\n",
      "Import ./model_2_channels/Fc_1.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 95.29%\n",
      "Inference time: 0 min 8.555837 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 8.555837 sec\n"
     ]
    }
   ],
   "source": [
    "model_2_channels.import_free_parameters(\"./model_2_channels\", ignore_not_exists=True)\n",
    "\n",
    "target = n2d2.target.Score(provider)\n",
    "provider.set_partition('Test')\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_2_channels.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "target.log_confusion_matrix(\"model_2_channels\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_2_channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd4c858-8167-49a8-a319-f269bda293df",
   "metadata": {},
   "source": [
    "## N-MNIST - 8 bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25b4b603-bddf-484e-b3ef-d69ab1684799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Loading Model (8 bit quantization) ###\n",
      "'Sequence_1' Sequence(\n",
      "\t(0): 'Conv_3' Conv(Frame<float>)(nb_inputs=2, nb_outputs=6, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc540200ab0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=255))\n",
      "\t(1): 'Pool2d_2' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(2): 'Conv_4' Conv(Frame<float>)(nb_inputs=6, nb_outputs=16, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc59afde970>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=255))\n",
      "\t(3): 'Pool2d_3' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(4): 'Conv_5' Conv(Frame<float>)(nb_inputs=16, nb_outputs=120, kernel_dims=[4, 4], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc5401fb5f0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=255))\n",
      "\t(5): 'Fc_2' Fc(Frame<float>)(nb_inputs=120, nb_outputs=84 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc5401fb4b0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=255))\n",
      "\t(6): 'Fc_3' Fc(Frame<float>)(nb_inputs=84, nb_outputs=10 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Linear(clipping=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc54020f730>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=255))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model 8 bit quantization \n",
    "\n",
    "print(\"\\n### Loading Model (8 bit quantization) ###\")\n",
    "model_2_channels_quant_8bit = n2d2.cells.Sequence([\n",
    "    Conv(2, 6, kernel_dims=[5, 5], **conv_quantization_conf(8)),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_quantization_conf(8)),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [4, 4], **conv_quantization_conf(8)),\n",
    "    Fc(120, 84, **fc_quantization_conf1(8)),\n",
    "    Fc(84, 10, **fc_quantization_conf2(8)),\n",
    "])\n",
    "print(model_2_channels_quant_8bit)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb6b21-4b99-4691-83c4-6b218f991f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "target = n2d2.target.Score(provider)\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_2_channels_quant_8bit.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_2_channels_quant_8bit(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_2_channels_quant_8bit.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_8bit(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "\n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13984e3a-3468-428d-85d9-f1f13427f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_2_channels_quant_8bit/Conv_3.syntxt\n",
      "Import ./model_2_channels_quant_8bit/Conv_4.syntxt\n",
      "Import ./model_2_channels_quant_8bit/Conv_5.syntxt\n",
      "Import ./model_2_channels_quant_8bit/Fc_2.syntxt\n",
      "Import ./model_2_channels_quant_8bit/Fc_3.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 90.78%\n",
      "Inference time: 0 min 10.741923 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 10.741923 sec\n"
     ]
    }
   ],
   "source": [
    "model_2_channels_quant_8bit.import_free_parameters(\"./model_2_channels_quant_8bit\", ignore_not_exists=True)\n",
    "\n",
    "provider.set_partition('Test')\n",
    "target = n2d2.target.Score(provider)\n",
    "\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_2_channels_quant_8bit.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_8bit(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "target.log_confusion_matrix(\"model_2_channels_quant_8bit\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_2_channels_quant_8bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be79a6-3711-41dc-afd5-3faac5b63f36",
   "metadata": {},
   "source": [
    "## N-MNIST - 4 bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d032012a-d090-4f45-a2c0-7267a46f25de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Loading Model (4 bit quantization) ###\n",
      "'Sequence_2' Sequence(\n",
      "\t(0): 'Conv_6' Conv(Frame<float>)(nb_inputs=2, nb_outputs=6, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc55f1cbcb0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=15))\n",
      "\t(1): 'Pool2d_4' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(2): 'Conv_7' Conv(Frame<float>)(nb_inputs=6, nb_outputs=16, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc54020d7f0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=15))\n",
      "\t(3): 'Pool2d_5' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(4): 'Conv_8' Conv(Frame<float>)(nb_inputs=16, nb_outputs=120, kernel_dims=[4, 4], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc54020da30>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=15))\n",
      "\t(5): 'Fc_4' Fc(Frame<float>)(nb_inputs=120, nb_outputs=84 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc54020eaf0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=15))\n",
      "\t(6): 'Fc_5' Fc(Frame<float>)(nb_inputs=84, nb_outputs=10 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Linear(clipping=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc599ec4b30>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=15))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model 4 bit quantization \n",
    "\n",
    "print(\"\\n### Loading Model (4 bit quantization) ###\")\n",
    "model_2_channels_quant_4bit = n2d2.cells.Sequence([\n",
    "    Conv(2, 6, kernel_dims=[5, 5], **conv_quantization_conf(4)),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_quantization_conf(4)),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [4, 4], **conv_quantization_conf(4)),\n",
    "    Fc(120, 84, **fc_quantization_conf1(4)),\n",
    "    Fc(84, 10, **fc_quantization_conf2(4)),\n",
    "])\n",
    "print(model_2_channels_quant_4bit)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d406b09-0398-4cf2-b8d2-16305aca0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "target = n2d2.target.Score(provider)\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_2_channels_quant_4bit.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_2_channels_quant_4bit(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_2_channels_quant_4bit.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_4bit(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "\n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7ef8016-f72b-4b80-ae30-ae0a46723eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_2_channels_quant_4bit/Conv_6.syntxt\n",
      "Import ./model_2_channels_quant_4bit/Conv_7.syntxt\n",
      "Import ./model_2_channels_quant_4bit/Conv_8.syntxt\n",
      "Import ./model_2_channels_quant_4bit/Fc_4.syntxt\n",
      "Import ./model_2_channels_quant_4bit/Fc_5.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 91.71%\n",
      "Inference time: 0 min 11.774999 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 11.774999 sec\n"
     ]
    }
   ],
   "source": [
    "model_2_channels_quant_4bit.import_free_parameters(\"./model_2_channels_quant_4bit\", ignore_not_exists=True)\n",
    "\n",
    "provider.set_partition('Test')\n",
    "target = n2d2.target.Score(provider)\n",
    "\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_2_channels_quant_4bit.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_4bit(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "target.log_confusion_matrix(\"model_2_channels_quant_4bit\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_2_channels_quant_4bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7940dd97-8d88-4c01-983d-2b76a5032b89",
   "metadata": {},
   "source": [
    "## N-MNIST - 2 bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0bc5eaf-2fa6-4225-8700-e27fae4f9929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Loading Model (2 bit quantization) ###\n",
      "'Sequence_3' Sequence(\n",
      "\t(0): 'Conv_9' Conv(Frame<float>)(nb_inputs=2, nb_outputs=6, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc54020ff30>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=3))\n",
      "\t(1): 'Pool2d_6' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(2): 'Conv_10' Conv(Frame<float>)(nb_inputs=6, nb_outputs=16, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc5401eabf0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=3))\n",
      "\t(3): 'Pool2d_7' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(4): 'Conv_11' Conv(Frame<float>)(nb_inputs=16, nb_outputs=120, kernel_dims=[4, 4], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc55f1a1870>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=3))\n",
      "\t(5): 'Fc_6' Fc(Frame<float>)(nb_inputs=120, nb_outputs=84 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc53fd28470>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=3))\n",
      "\t(6): 'Fc_7' Fc(Frame<float>)(nb_inputs=84, nb_outputs=10 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Linear(clipping=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc53fd29630>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=3))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model 2 bit quantization \n",
    "\n",
    "print(\"\\n### Loading Model (2 bit quantization) ###\")\n",
    "model_2_channels_quant_2bit = n2d2.cells.Sequence([\n",
    "    Conv(2, 6, kernel_dims=[5, 5], **conv_quantization_conf(2)),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_quantization_conf(2)),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [4, 4], **conv_quantization_conf(2)),\n",
    "    Fc(120, 84, **fc_quantization_conf1(2)),\n",
    "    Fc(84, 10, **fc_quantization_conf2(2)),\n",
    "])\n",
    "print(model_2_channels_quant_2bit)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88509629-3594-4104-9cb2-185680debaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "target = n2d2.target.Score(provider)\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_2_channels_quant_2bit.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_2_channels_quant_2bit(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_2_channels_quant_2bit.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_2bit(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "\n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34825a21-6376-44e2-8f2f-6ec45ab7117e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_2_channels_quant_2bit/Conv_9.syntxt\n",
      "Import ./model_2_channels_quant_2bit/Conv_10.syntxt\n",
      "Import ./model_2_channels_quant_2bit/Conv_11.syntxt\n",
      "Import ./model_2_channels_quant_2bit/Fc_6.syntxt\n",
      "Import ./model_2_channels_quant_2bit/Fc_7.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 85.33%\n",
      "Inference time: 0 min 10.658541 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 10.658541 sec\n"
     ]
    }
   ],
   "source": [
    "model_2_channels_quant_2bit.import_free_parameters(\"./model_2_channels_quant_2bit\", ignore_not_exists=True)\n",
    "\n",
    "provider.set_partition('Test')\n",
    "target = n2d2.target.Score(provider)\n",
    "\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_2_channels_quant_2bit.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_2bit(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "target.log_confusion_matrix(\"model_2_channels_quant_2bit\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_2_channels_quant_2bit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e637c433-f946-4507-a35f-5949a17a20a6",
   "metadata": {},
   "source": [
    "## N-MNIST -  1 bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba7cb039-1424-4a6d-8207-792ceff93159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Loading Model (1 bit quantization) ###\n",
      "'Sequence_4' Sequence(\n",
      "\t(0): 'Conv_12' Conv(Frame<float>)(nb_inputs=2, nb_outputs=6, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc53e7691b0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=1))\n",
      "\t(1): 'Pool2d_8' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(2): 'Conv_13' Conv(Frame<float>)(nb_inputs=6, nb_outputs=16, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc53e769ef0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=1))\n",
      "\t(3): 'Pool2d_9' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(4): 'Conv_14' Conv(Frame<float>)(nb_inputs=16, nb_outputs=120, kernel_dims=[4, 4], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc53e7c27b0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=1))\n",
      "\t(5): 'Fc_8' Fc(Frame<float>)(nb_inputs=120, nb_outputs=84 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc55ca03930>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=1))\n",
      "\t(6): 'Fc_9' Fc(Frame<float>)(nb_inputs=84, nb_outputs=10 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Linear(clipping=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7fc54020e2f0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model 1 bit quantization\n",
    "\n",
    "print(\"\\n### Loading Model (1 bit quantization) ###\")\n",
    "model_2_channels_quant_1bit = n2d2.cells.Sequence([\n",
    "    Conv(2, 6, kernel_dims=[5, 5], **conv_quantization_conf(1)),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_quantization_conf(1)),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [4, 4], **conv_quantization_conf(1)),\n",
    "    Fc(120, 84, **fc_quantization_conf1(1)),\n",
    "    Fc(84, 10, **fc_quantization_conf2(1)),\n",
    "])\n",
    "print(model_2_channels_quant_1bit)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ad7982-43d1-4421-ad8e-c55ffe1e0c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "target = n2d2.target.Score(provider)\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_2_channels_quant_1bit.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_2_channels_quant_1bit(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_2_channels_quant_1bit.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_1bit(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "\n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "595e0569-ebf7-455d-b5b2-5b0bb5d37913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_2_channels_quant_1bit/Conv_12.syntxt\n",
      "Import ./model_2_channels_quant_1bit/Conv_13.syntxt\n",
      "Import ./model_2_channels_quant_1bit/Conv_14.syntxt\n",
      "Import ./model_2_channels_quant_1bit/Fc_8.syntxt\n",
      "Import ./model_2_channels_quant_1bit/Fc_9.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 81.09%\n",
      "Inference time: 0 min 10.669835 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 10.669835 sec\n"
     ]
    }
   ],
   "source": [
    "model_2_channels_quant_1bit.import_free_parameters(\"./model_2_channels_quant_1bit\", ignore_not_exists=True)\n",
    "\n",
    "provider.set_partition('Test')\n",
    "target = n2d2.target.Score(provider)\n",
    "\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_2_channels_quant_1bit.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_1bit(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "target.log_confusion_matrix(\"model_2_channels_quant_1bit\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_2_channels_quant_1bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049e6d19-8fc8-4b61-8573-15908da1732c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in model_2_channels_quant_1bit.get_cells().values():\n",
    "     try:\n",
    "        weights = cell.get_weights()\n",
    "        print(weights)\n",
    "     except AttributeError as e:\n",
    "        print(f\"Attention: {cell} Error: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72591c47-dd4b-471a-96c2-7124f50addb9",
   "metadata": {},
   "source": [
    "# Model 2 (Deeper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2d61d21-2505-4f47-b1c0-2b0a1b24f339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Loading Model (without quantization) ###\n",
      "'Sequence_0' Sequence(\n",
      "\t(0): 'Conv_0' Conv(Frame<float>)(nb_inputs=2, nb_outputs=6, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f2720f6e4f0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=None)\n",
      "\t(1): 'Pool2d_0' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(2): 'Conv_1' Conv(Frame<float>)(nb_inputs=6, nb_outputs=16, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f27231b7fb0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=None)\n",
      "\t(3): 'Conv_2' Conv(Frame<float>)(nb_inputs=16, nb_outputs=120, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26e6265e70>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=None)\n",
      "\t(4): 'Conv_3' Conv(Frame<float>)(nb_inputs=120, nb_outputs=240, kernel_dims=[4, 4], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f2720efab70>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=None)\n",
      "\t(5): 'Fc_0' Fc(Frame<float>)(nb_inputs=240, nb_outputs=120 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f2720f7a4f0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=None)\n",
      "\t(6): 'Fc_1' Fc(Frame<float>)(nb_inputs=120, nb_outputs=84 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f2720f5bf30>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=None)\n",
      "\t(7): 'Fc_2' Fc(Frame<float>)(nb_inputs=84, nb_outputs=10 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Linear(clipping=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f2720f9ec70>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n### Loading Model (without quantization) ###\")\n",
    "model_2_channels_deeper = n2d2.cells.Sequence([\n",
    "    Conv(2, 6, kernel_dims=[5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_conf()),\n",
    "    #Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [5, 5], **conv_conf()),\n",
    "    Conv(120, 240, [4, 4], **conv_conf()), # New Conv Layer\n",
    "    Fc(240, 120, **fc_conf1()), # New Fc Layer\n",
    "    Fc(120, 84, **fc_conf1()),\n",
    "    Fc(84, 10, **fc_conf2()),\n",
    "])\n",
    "print(model_2_channels_deeper)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06127c69-176a-4410-8449-4185352bd8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_2_channels_deeper.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_2_channels_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_2_channels_deeper.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "        \n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5d77a8e-42da-4e93-9421-e6e3deeab11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_2_channels_deeper/Conv_0.syntxt\n",
      "Import ./model_2_channels_deeper/Conv_1.syntxt\n",
      "Import ./model_2_channels_deeper/Conv_2.syntxt\n",
      "Import ./model_2_channels_deeper/Conv_3.syntxt\n",
      "Import ./model_2_channels_deeper/Fc_0.syntxt\n",
      "Import ./model_2_channels_deeper/Fc_1.syntxt\n",
      "Import ./model_2_channels_deeper/Fc_2.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 95.81%\n",
      "Inference time: 0 min 17.252433 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 17.252433 sec\n"
     ]
    }
   ],
   "source": [
    "model_2_channels_deeper.import_free_parameters(\"./model_2_channels_deeper\", ignore_not_exists=True)\n",
    "\n",
    "target = n2d2.target.Score(provider)\n",
    "provider.set_partition('Test')\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_2_channels_deeper.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "target.log_confusion_matrix(\"model_2_channels_deeper\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_2_channels_deeper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98920027-a209-4aa8-8f95-3d2df909ea3c",
   "metadata": {},
   "source": [
    "## N-MNIST - 8 bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf78328f-3a62-4e1b-b61b-603fee320a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Loading Model (8 bit quantization) ###\n",
      "'Sequence_1' Sequence(\n",
      "\t(0): 'Conv_4' Conv(Frame<float>)(nb_inputs=2, nb_outputs=6, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c72a7eb0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=255))\n",
      "\t(1): 'Pool2d_1' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(2): 'Conv_5' Conv(Frame<float>)(nb_inputs=6, nb_outputs=16, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c729d570>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=255))\n",
      "\t(3): 'Conv_6' Conv(Frame<float>)(nb_inputs=16, nb_outputs=120, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f2720f6fcb0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=255))\n",
      "\t(4): 'Conv_7' Conv(Frame<float>)(nb_inputs=120, nb_outputs=240, kernel_dims=[4, 4], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26e6258070>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=255))\n",
      "\t(5): 'Fc_3' Fc(Frame<float>)(nb_inputs=240, nb_outputs=120 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c5820cf0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=255))\n",
      "\t(6): 'Fc_4' Fc(Frame<float>)(nb_inputs=120, nb_outputs=84 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c5822330>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=255))\n",
      "\t(7): 'Fc_5' Fc(Frame<float>)(nb_inputs=84, nb_outputs=10 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Linear(clipping=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c5822b70>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=255))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model 8 bit quantization \n",
    "\n",
    "print(\"\\n### Loading Model (8 bit quantization) ###\")\n",
    "model_2_channels_quant_8bit_deeper = n2d2.cells.Sequence([\n",
    "    Conv(2, 6, kernel_dims=[5, 5], **conv_quantization_conf(8)),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_quantization_conf(8)),\n",
    "    #Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [5, 5], **conv_quantization_conf(8)),\n",
    "    Conv(120, 240, [4, 4], **conv_quantization_conf(8)), # New Conv Layer\n",
    "    Fc(240, 120, **fc_quantization_conf1(8)), # New Fc Layer\n",
    "    Fc(120, 84, **fc_quantization_conf1(8)),\n",
    "    Fc(84, 10, **fc_quantization_conf2(8)),\n",
    "])\n",
    "print(model_2_channels_quant_8bit_deeper)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1b99e2-96af-4a70-b8fc-dc668c025003",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "target = n2d2.target.Score(provider)\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_2_channels_quant_8bit_deeper.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_2_channels_quant_8bit_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_2_channels_quant_8bit_deeper.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_8bit_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "\n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "871835c8-0157-4773-ad2e-aa0459e875a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_2_channels_quant_8bit_deeper/Conv_4.syntxt\n",
      "Import ./model_2_channels_quant_8bit_deeper/Conv_5.syntxt\n",
      "Import ./model_2_channels_quant_8bit_deeper/Conv_6.syntxt\n",
      "Import ./model_2_channels_quant_8bit_deeper/Conv_7.syntxt\n",
      "Import ./model_2_channels_quant_8bit_deeper/Fc_3.syntxt\n",
      "Import ./model_2_channels_quant_8bit_deeper/Fc_4.syntxt\n",
      "Import ./model_2_channels_quant_8bit_deeper/Fc_5.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 96.91%\n",
      "Inference time: 0 min 23.162939 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 23.162939 sec\n"
     ]
    }
   ],
   "source": [
    "model_2_channels_quant_8bit_deeper.import_free_parameters(\"./model_2_channels_quant_8bit_deeper\", ignore_not_exists=True)\n",
    "\n",
    "provider.set_partition('Test')\n",
    "target = n2d2.target.Score(provider)\n",
    "\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_2_channels_quant_8bit_deeper.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_8bit_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "target.log_confusion_matrix(\"model_2_channels_quant_8bit_deeper\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_2_channels_quant_8bit_deeper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfe186c-ef10-474c-b77b-640b440c4519",
   "metadata": {},
   "source": [
    "## N-MNIST - 4 bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6614e448-dde9-49a4-8d17-af51921ddd8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Loading Model (4 bit quantization) ###\n",
      "'Sequence_2' Sequence(\n",
      "\t(0): 'Conv_8' Conv(Frame<float>)(nb_inputs=2, nb_outputs=6, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c72941f0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=15))\n",
      "\t(1): 'Pool2d_2' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(2): 'Conv_9' Conv(Frame<float>)(nb_inputs=6, nb_outputs=16, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c7295530>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=15))\n",
      "\t(3): 'Conv_10' Conv(Frame<float>)(nb_inputs=16, nb_outputs=120, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f2720f663b0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=15))\n",
      "\t(4): 'Conv_11' Conv(Frame<float>)(nb_inputs=120, nb_outputs=240, kernel_dims=[4, 4], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c567fbb0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=15))\n",
      "\t(5): 'Fc_6' Fc(Frame<float>)(nb_inputs=240, nb_outputs=120 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c57d0cb0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=15))\n",
      "\t(6): 'Fc_7' Fc(Frame<float>)(nb_inputs=120, nb_outputs=84 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c57d1630>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=15))\n",
      "\t(7): 'Fc_8' Fc(Frame<float>)(nb_inputs=84, nb_outputs=10 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Linear(clipping=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c57d1f70>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=15))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model 4 bit quantization \n",
    "\n",
    "print(\"\\n### Loading Model (4 bit quantization) ###\")\n",
    "model_2_channels_quant_4bit_deeper = n2d2.cells.Sequence([\n",
    "    Conv(2, 6, kernel_dims=[5, 5], **conv_quantization_conf(4)),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_quantization_conf(4)),\n",
    "    #Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [5, 5], **conv_quantization_conf(4)),\n",
    "    Conv(120, 240, [4, 4], **conv_quantization_conf(4)), # New Conv Layer\n",
    "    Fc(240, 120, **fc_quantization_conf1(4)), # New Fc Layer\n",
    "    Fc(120, 84, **fc_quantization_conf1(4)),\n",
    "    Fc(84, 10, **fc_quantization_conf2(4)),\n",
    "])\n",
    "print(model_2_channels_quant_4bit_deeper)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf3dff8-4bd9-4862-9489-1138f1643667",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "target = n2d2.target.Score(provider)\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_2_channels_quant_4bit_deeper.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_2_channels_quant_4bit_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_2_channels_quant_4bit_deeper.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_4bit_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "\n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1de9e2c-7868-4ffc-ac3d-afcfc2e136f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_2_channels_quant_4bit_deeper/Conv_8.syntxt\n",
      "Import ./model_2_channels_quant_4bit_deeper/Conv_9.syntxt\n",
      "Import ./model_2_channels_quant_4bit_deeper/Conv_10.syntxt\n",
      "Import ./model_2_channels_quant_4bit_deeper/Conv_11.syntxt\n",
      "Import ./model_2_channels_quant_4bit_deeper/Fc_6.syntxt\n",
      "Import ./model_2_channels_quant_4bit_deeper/Fc_7.syntxt\n",
      "Import ./model_2_channels_quant_4bit_deeper/Fc_8.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 95.82%\n",
      "Inference time: 0 min 23.26026 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 23.26026 sec\n"
     ]
    }
   ],
   "source": [
    "model_2_channels_quant_4bit_deeper.import_free_parameters(\"./model_2_channels_quant_4bit_deeper\", ignore_not_exists=True)\n",
    "\n",
    "provider.set_partition('Test')\n",
    "target = n2d2.target.Score(provider)\n",
    "\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_2_channels_quant_4bit_deeper.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_4bit_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "target.log_confusion_matrix(\"model_2_channels_quant_4bit_deeper\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_2_channels_quant_4bit_deeper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b646193-a1fd-4028-850a-6842944dba7f",
   "metadata": {},
   "source": [
    "## N-MNIST - 2 bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "441d05c2-27b0-49cb-99bc-37d527f93bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Loading Model (2 bit quantization) ###\n",
      "'Sequence_3' Sequence(\n",
      "\t(0): 'Conv_12' Conv(Frame<float>)(nb_inputs=2, nb_outputs=6, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c57d1b30>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=3))\n",
      "\t(1): 'Pool2d_3' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(2): 'Conv_13' Conv(Frame<float>)(nb_inputs=6, nb_outputs=16, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c57d34f0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=3))\n",
      "\t(3): 'Conv_14' Conv(Frame<float>)(nb_inputs=16, nb_outputs=120, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c7296030>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=3))\n",
      "\t(4): 'Conv_15' Conv(Frame<float>)(nb_inputs=120, nb_outputs=240, kernel_dims=[4, 4], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c72966f0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=3))\n",
      "\t(5): 'Fc_9' Fc(Frame<float>)(nb_inputs=240, nb_outputs=120 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c7233c70>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=3))\n",
      "\t(6): 'Fc_10' Fc(Frame<float>)(nb_inputs=120, nb_outputs=84 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c58082f0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=3))\n",
      "\t(7): 'Fc_11' Fc(Frame<float>)(nb_inputs=84, nb_outputs=10 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Linear(clipping=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c5808ef0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=3))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model 2 bit quantization \n",
    "\n",
    "print(\"\\n### Loading Model (2 bit quantization) ###\")\n",
    "model_2_channels_quant_2bit_deeper = n2d2.cells.Sequence([\n",
    "    Conv(2, 6, kernel_dims=[5, 5], **conv_quantization_conf(2)),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_quantization_conf(2)),\n",
    "    #Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [5, 5], **conv_quantization_conf(2)),\n",
    "    Conv(120, 240, [4, 4], **conv_quantization_conf(2)), # New Conv Layer\n",
    "    Fc(240, 120, **fc_quantization_conf1(2)), # New Fc Layer\n",
    "    Fc(120, 84, **fc_quantization_conf1(2)),\n",
    "    Fc(84, 10, **fc_quantization_conf2(2)),\n",
    "])\n",
    "print(model_2_channels_quant_2bit_deeper)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1dc48-aeed-4986-a8de-ac450516a0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "target = n2d2.target.Score(provider)\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_2_channels_quant_2bit_deeper.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_2_channels_quant_2bit_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_2_channels_quant_2bit_deeper.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_2bit_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "\n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e705900a-bab6-45a5-8a43-6dd65018b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_2_channels_quant_2bit_deeper/Conv_12.syntxt\n",
      "Import ./model_2_channels_quant_2bit_deeper/Conv_13.syntxt\n",
      "Import ./model_2_channels_quant_2bit_deeper/Conv_14.syntxt\n",
      "Import ./model_2_channels_quant_2bit_deeper/Conv_15.syntxt\n",
      "Import ./model_2_channels_quant_2bit_deeper/Fc_9.syntxt\n",
      "Import ./model_2_channels_quant_2bit_deeper/Fc_10.syntxt\n",
      "Import ./model_2_channels_quant_2bit_deeper/Fc_11.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 94.49%\n",
      "Inference time: 0 min 34.418393 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 34.418393 sec\n"
     ]
    }
   ],
   "source": [
    "model_2_channels_quant_2bit_deeper.import_free_parameters(\"./model_2_channels_quant_2bit_deeper\", ignore_not_exists=True)\n",
    "\n",
    "provider.set_partition('Test')\n",
    "target = n2d2.target.Score(provider)\n",
    "\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_2_channels_quant_2bit_deeper.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_2bit_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "target.log_confusion_matrix(\"model_2_channels_quant_2bit_deeper\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_2_channels_quant_2bit_deeper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6a3981-2c40-4dd2-ab6f-9e3d1f427435",
   "metadata": {},
   "source": [
    "## N-MNIST -  1 bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb39144f-bb8e-4797-8f3a-490d8cf59326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Loading Model (1 bit quantization) ###\n",
      "'Sequence_4' Sequence(\n",
      "\t(0): 'Conv_16' Conv(Frame<float>)(nb_inputs=2, nb_outputs=6, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c580af70>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=1))\n",
      "\t(1): 'Pool2d_4' Pool2d(Frame<float>)(pool_dims=[2, 2], stride_dims=[2, 2], pooling=Pooling.Average | activation=None)\n",
      "\t(2): 'Conv_17' Conv(Frame<float>)(nb_inputs=6, nb_outputs=16, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c72305f0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=1))\n",
      "\t(3): 'Conv_18' Conv(Frame<float>)(nb_inputs=16, nb_outputs=120, kernel_dims=[5, 5], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c7230d70>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=1))\n",
      "\t(4): 'Conv_19' Conv(Frame<float>)(nb_inputs=120, nb_outputs=240, kernel_dims=[4, 4], sub_sample_dims=[1, 1], stride_dims=[1, 1], padding_dims=[0, 0], dilation_dims=[1, 1] | back_propagate=True, no_bias=True, outputs_remap=, weights_export_flip=False, weights_export_format=OCHW, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c729c030>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=1))\n",
      "\t(5): 'Fc_12' Fc(Frame<float>)(nb_inputs=240, nb_outputs=120 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c61b4e30>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=1))\n",
      "\t(6): 'Fc_13' Fc(Frame<float>)(nb_inputs=120, nb_outputs=84 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Rectifier(clipping=0.0, leak_slope=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c61b66f0>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=1))\n",
      "\t(7): 'Fc_14' Fc(Frame<float>)(nb_inputs=84, nb_outputs=10 | back_propagate=True, drop_connect=1.0, no_bias=True, normalize=False, outputs_remap=, weights_export_format=OC, activation=Linear(clipping=0.0, quantizer=None, scaling=<N2D2.Scaling object at 0x7f26c61b7530>), weights_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), bias_solver=Adam(beta1=0.9, beta2=0.999, clamping=, epsilon=1e-08, learning_rate=0.001), weights_filler=Normal(mean=0.0, std_dev=0.05), bias_filler=Normal(mean=0.0, std_dev=0.05), quantizer=SATCell(apply_quantization=True, apply_scaling=True, quant_mode=Default, range=1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Model 1 bit quantization\n",
    "\n",
    "print(\"\\n### Loading Model (1 bit quantization) ###\")\n",
    "model_2_channels_quant_1bit_deeper = n2d2.cells.Sequence([\n",
    "    Conv(2, 6, kernel_dims=[5, 5], **conv_quantization_conf(1)),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_quantization_conf(1)),\n",
    "    #Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [5, 5], **conv_quantization_conf(1)),\n",
    "    Conv(120, 240, [4, 4], **conv_quantization_conf(1)), # New Conv Layer\n",
    "    Fc(240, 120, **fc_quantization_conf1(1)), # New Fc Layer\n",
    "    Fc(120, 84, **fc_quantization_conf1(1)),\n",
    "    Fc(84, 10, **fc_quantization_conf2(1)),\n",
    "])\n",
    "print(model_2_channels_quant_1bit_deeper)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6b3f4-7fc8-467a-b543-d1ad11c6cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "target = n2d2.target.Score(provider)\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_2_channels_quant_1bit_deeper.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_2_channels_quant_1bit_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_2_channels_quant_1bit_deeper.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_1bit_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "\n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "383bd912-addd-4c01-8864-6bf6808b1b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_2_channels_quant_1bit_deeper/Conv_16.syntxt\n",
      "Import ./model_2_channels_quant_1bit_deeper/Conv_17.syntxt\n",
      "Import ./model_2_channels_quant_1bit_deeper/Conv_18.syntxt\n",
      "Import ./model_2_channels_quant_1bit_deeper/Conv_19.syntxt\n",
      "Import ./model_2_channels_quant_1bit_deeper/Fc_12.syntxt\n",
      "Import ./model_2_channels_quant_1bit_deeper/Fc_13.syntxt\n",
      "Import ./model_2_channels_quant_1bit_deeper/Fc_14.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 93.31%\n",
      "Inference time: 0 min 34.742852 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 34.742852 sec\n"
     ]
    }
   ],
   "source": [
    "model_2_channels_quant_1bit_deeper.import_free_parameters(\"./model_2_channels_quant_1bit_deeper\", ignore_not_exists=True)\n",
    "\n",
    "provider.set_partition('Test')\n",
    "target = n2d2.target.Score(provider)\n",
    "\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_2_channels_quant_1bit_deeper.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels_quant_1bit_deeper(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "target.log_confusion_matrix(\"model_2_channels_quant_1bit_deeper\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_2_channels_quant_1bit_deeper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aa2143-f953-46ed-a9e4-6e8abc191cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cell in model_2_channels_quant_1bit_deeper.get_cells().values():\n",
    "     try:\n",
    "        weights = cell.get_weights()\n",
    "        print(weights)\n",
    "     except AttributeError as e:\n",
    "        print(f\"Attention: {cell} Error: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ebeb66-e51b-4e3b-8195-1d43e9402a86",
   "metadata": {},
   "source": [
    "# Clean and rename files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a5a268b-8421-4e09-9988-9fb254f9cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_rename_files(folder_path):\n",
    "    try:\n",
    "        # List all files in the folder\n",
    "        files = os.listdir(folder_path)\n",
    "        \n",
    "        # Delete files that do not contain \"quant\" in the name\n",
    "        for file_name in files:\n",
    "            if \"quant\" not in file_name:\n",
    "                file_path = os.path.join(folder_path, file_name)\n",
    "                os.remove(file_path)\n",
    "                \n",
    "        # Rename the remaining files by removing all occurrences of \"_quant\"\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            if \"quant\" in file_name:\n",
    "                new_name = file_name.replace(\"_quant\", \"\")\n",
    "                old_path = os.path.join(folder_path, file_name)\n",
    "                new_path = os.path.join(folder_path, new_name)\n",
    "                os.rename(old_path, new_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c2ef1eb-5009-426f-8518-1d74eee88dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean_and_rename_files(\"./model_2_channels_quant_8bit_deeper\")\n",
    "#clean_and_rename_files(\"./model_2_channels_quant_8bit\")\n",
    "#clean_and_rename_files(\"./model_2_channels_quant_4bit_deeper\")\n",
    "#clean_and_rename_files(\"./model_2_channels_quant_4bit\")\n",
    "#clean_and_rename_files(\"./model_2_channels_quant_2bit_deeper\")\n",
    "#clean_and_rename_files(\"./model_2_channels_quant_2bit\")\n",
    "#clean_and_rename_files(\"./model_2_channels_quant_1bit_deeper\")\n",
    "#clean_and_rename_files(\"./model_2_channels_quant_1bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7ee63c-0896-44b4-b93a-55605c79e6c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
