{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c0cbaa-e1f8-42de-bb65-e425966866de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import n2d2\n",
    "from n2d2.cells.nn import Fc, Conv, Pool2d\n",
    "import math\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ffb025-f52c-4af3-ab0a-70dbca2ec322",
   "metadata": {},
   "source": [
    "## N-MNIST (Accumulation 300 events)\n",
    "\n",
    "| Representation | Accuracy |\n",
    "| ------------------- | -------- |\n",
    "| 2 Channels             | 95.29%      | \n",
    "| 3 Channels             | 94.50%      | \n",
    "| positive + negative              | 93.98%      |\n",
    "| positive - negative            | 94.85%      | \n",
    "| only positive            | 93.21%      | \n",
    "| only negative            | 93.78%      |\n",
    "\n",
    "Note: Initially, we kept the input dimensions at 28x28. However, after opting for the 2-channel representation, we adjusted the input size to 32x32. In the latter scenario, as detailed in the other Jupyter notebook file, we achieved an accuracy of 95.43% instead of 95.29%.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f86c596f-d458-4f38-abd5-3a13f6e812b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver_conf = n2d2.ConfigSection(\n",
    "    learning_rate=0.001,\n",
    ")\n",
    "\n",
    "def conv_conf():\n",
    "    return n2d2.ConfigSection(\n",
    "        activation=n2d2.activation.Rectifier(),\n",
    "        no_bias=True,\n",
    "        weights_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        bias_solver=n2d2.solver.Adam(**solver_conf),\n",
    "    )\n",
    "    \n",
    "def fc_conf1():\n",
    "    return n2d2.ConfigSection(\n",
    "        activation=n2d2.activation.Rectifier(),\n",
    "        no_bias=True,\n",
    "        weights_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        bias_solver=n2d2.solver.Adam(**solver_conf),\n",
    "    )\n",
    "def fc_conf2():\n",
    "    return n2d2.ConfigSection(\n",
    "        activation=n2d2.activation.Linear(),\n",
    "        no_bias=True,\n",
    "        weights_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        bias_solver=n2d2.solver.Adam(**solver_conf),\n",
    "    )\n",
    "    \n",
    "# Definition of layers for quantization\n",
    "\n",
    "def conv_quantization_conf(n_bits):\n",
    "    return n2d2.ConfigSection(\n",
    "        activation=n2d2.activation.Rectifier(),\n",
    "        no_bias=True,\n",
    "        weights_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        bias_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        quantizer=n2d2.quantizer.SATCell(\n",
    "            apply_scaling=True,\n",
    "            apply_quantization=True,\n",
    "            range=2**n_bits-1,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def fc_quantization_conf1(n_bits):\n",
    "    return n2d2.ConfigSection(\n",
    "        activation=n2d2.activation.Rectifier(),\n",
    "        no_bias=True,\n",
    "        weights_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        bias_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        quantizer=n2d2.quantizer.SATCell(\n",
    "            apply_scaling=True,\n",
    "            apply_quantization=True,\n",
    "            range=2**n_bits-1,\n",
    "        ),\n",
    "    )\n",
    "def fc_quantization_conf2(n_bits):\n",
    "    return n2d2.ConfigSection(\n",
    "        activation=n2d2.activation.Linear(),\n",
    "        no_bias=True,\n",
    "        weights_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        bias_solver=n2d2.solver.Adam(**solver_conf),\n",
    "        quantizer=n2d2.quantizer.SATCell(\n",
    "            apply_scaling=True,\n",
    "            apply_quantization=True,\n",
    "            range=2**n_bits-1,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dc8fb1-8fc4-4dd2-bc07-df860c3be44f",
   "metadata": {},
   "source": [
    "## N-MNIST : 2 channels (positive events, negative events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b879fc01-1ff5-44ca-a456-7f8f9f9d15e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_final/data_2_channels.pkl', 'rb') as file:\n",
    "    data_2_channels = pickle.load(file)\n",
    "with open('data_final/labels_2_channels.pkl', 'rb') as file:\n",
    "    labels_2_channels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd8ad2-3e5f-49a7-8958-9c6443f1ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = n2d2.database.Numpy(random_partitioning=False)\n",
    "\n",
    "db.load(data_2_channels, labels_2_channels)\n",
    "db.partition_stimuli(5/7, 1/7, 1/7) # training: 50k, validation: 10k, test: 10k\n",
    "\n",
    "print(\"\\n### Create Provider ###\")\n",
    "batch_size = 64\n",
    "provider = n2d2.provider.DataProvider(db, [28, 28, 2], batch_size=batch_size)\n",
    "print(provider)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af09a1e-a723-469b-a31f-0d4c3f446f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Loading Model (without quantization) ###\")\n",
    "model_2_channels = n2d2.cells.Sequence([\n",
    "    Conv(2, 6, kernel_dims=[5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [4, 4], **conv_conf()),\n",
    "    Fc(120, 84, **fc_conf1()),\n",
    "    Fc(84, 10, **fc_conf2()),\n",
    "])\n",
    "print(model_2_channels)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0ed69-2d29-4e43-be58-d1199c643e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_2_channels.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_2_channels(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_2_channels.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "        \n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "220da472-ebbe-496e-b129-24716a903ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_2_channels_old/Conv_0.syntxt\n",
      "Import ./model_2_channels_old/Conv_1.syntxt\n",
      "Import ./model_2_channels_old/Conv_2.syntxt\n",
      "Import ./model_2_channels_old/Fc_0.syntxt\n",
      "Import ./model_2_channels_old/Fc_1.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 95.29%\n",
      "Inference time: 0 min 4.672638 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 4.672638 sec\n"
     ]
    }
   ],
   "source": [
    "model_2_channels.import_free_parameters(\"./model_2_channels_old\", ignore_not_exists=True)\n",
    "target = n2d2.target.Score(provider)\n",
    "provider.set_partition('Test')\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_2_channels.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_2_channels(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "#target.log_confusion_matrix(\"model_2_channels_old\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_2_channels_old\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca2cb65-8053-4538-bde3-10a19cf4481e",
   "metadata": {},
   "source": [
    "## N-MNIST : 3 channels (positive events, negative events, average timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4351e6a-a88c-47b9-840f-d6961b6f5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_final/data_3_channels.pkl', 'rb') as file:\n",
    "    data_3_channels = pickle.load(file)\n",
    "with open('data_final/labels_3_channels.pkl', 'rb') as file:\n",
    "    labels_3_channels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977a2924-4163-4fe2-bc72-855b5bad1cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = n2d2.database.Numpy(random_partitioning=False)\n",
    "db.load(data_3_channels, labels_3_channels)\n",
    "db.partition_stimuli(5/7, 1/7, 1/7) # training: 50k, validation: 10k, test: 10k\n",
    "\n",
    "print(\"\\n### Create Provider ###\")\n",
    "batch_size = 64\n",
    "provider = n2d2.provider.DataProvider(db, [28, 28, 3], batch_size=batch_size)\n",
    "print(provider)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ffa62d-d408-49d1-99bd-65f71733b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Loading Model (without quantization) ###\")\n",
    "model_3_channels = n2d2.cells.Sequence([\n",
    "    Conv(3, 6, kernel_dims=[5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [4, 4], **conv_conf()),\n",
    "    Fc(120, 84, **fc_conf1()),\n",
    "    Fc(84, 10, **fc_conf2()),\n",
    "])\n",
    "print(model_3_channels)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8ea1bc-34a1-4005-9af7-da8b99ea6223",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_3_channels.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_3_channels(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_3_channels.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_3_channels(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "        \n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1901bd2-4a80-46e0-a0f8-4d2c832cf160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_3_channels/Conv_3.syntxt\n",
      "Import ./model_3_channels/Conv_4.syntxt\n",
      "Import ./model_3_channels/Conv_5.syntxt\n",
      "Import ./model_3_channels/Fc_2.syntxt\n",
      "Import ./model_3_channels/Fc_3.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 94.50%\n",
      "Inference time: 0 min 5.057639 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 5.057639 sec\n"
     ]
    }
   ],
   "source": [
    "model_3_channels.import_free_parameters(\"./model_3_channels\", ignore_not_exists=True)\n",
    "target = n2d2.target.Score(provider)\n",
    "provider.set_partition('Test')\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_3_channels.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_3_channels(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "#target.log_confusion_matrix(\"model_3_channels\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_3_channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f921596c-d21f-48e8-b20a-8dd3cca98f50",
   "metadata": {},
   "source": [
    "## N-MNIST : Positive Events + Negative Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "480c06ac-91e3-4527-bf9d-0a51aafe8f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_final/data_sum.pkl', 'rb') as file:\n",
    "    data_sum = pickle.load(file)\n",
    "with open('data_final/labels_sum.pkl', 'rb') as file:\n",
    "    labels_sum = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e732aeb-60b7-491f-813c-5d1a10ef4ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = n2d2.database.Numpy(random_partitioning=False)\n",
    "\n",
    "db.load(data_sum, labels_sum)\n",
    "db.partition_stimuli(5/7, 1/7, 1/7) # training: 50k, validation: 10k, test: 10k\n",
    "print(\"\\n### Create Provider ###\")\n",
    "batch_size = 64\n",
    "provider = n2d2.provider.DataProvider(db, [28, 28, 1], batch_size=batch_size)\n",
    "print(provider)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bba631-a964-4627-a084-ba3c35383f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Loading Model (without quantization) ###\")\n",
    "model_sum = n2d2.cells.Sequence([\n",
    "    Conv(1, 6, kernel_dims=[5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [4, 4], **conv_conf()),\n",
    "    Fc(120, 84, **fc_conf1()),\n",
    "    Fc(84, 10, **fc_conf2()),\n",
    "])\n",
    "print(model_sum)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3ecd92-f9b5-4d71-893c-d3f77ea221b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_sum.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_sum(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_sum.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_sum(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "\n",
    "print(\"\\n\")\n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5bef9aa-feab-4681-ad36-4e3beb46d251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_sum/Conv_6.syntxt\n",
      "Import ./model_sum/Conv_7.syntxt\n",
      "Import ./model_sum/Conv_8.syntxt\n",
      "Import ./model_sum/Fc_4.syntxt\n",
      "Import ./model_sum/Fc_5.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 93.98%\n",
      "Inference time: 0 min 4.308495 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 4.308495 sec\n"
     ]
    }
   ],
   "source": [
    "model_sum.import_free_parameters(\"./model_sum\", ignore_not_exists=True)\n",
    "target = n2d2.target.Score(provider)\n",
    "provider.set_partition('Test')\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_sum.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_sum(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "#target.log_confusion_matrix(\"model_sum\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_sum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4bc5b2-03d5-4beb-b009-dac94ebde521",
   "metadata": {},
   "source": [
    "## N-MNIST : Positive Events - Negative Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7a1bdb2-c7b9-4eb1-a180-0dbbe661b921",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_final/data_sub.pkl', 'rb') as file:\n",
    "    data_sub = pickle.load(file)\n",
    "with open('data_final/labels_sub.pkl', 'rb') as file:\n",
    "    labels_sub = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becd6e53-b441-4a79-8e14-74521385f00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = n2d2.database.Numpy(random_partitioning=False)\n",
    "\n",
    "db.load(data_sub, labels_sub)\n",
    "db.partition_stimuli(5/7, 1/7, 1/7) # training: 50k, validation: 10k, test: 10k\n",
    "\n",
    "print(\"\\n### Create Provider ###\")\n",
    "batch_size = 64\n",
    "provider = n2d2.provider.DataProvider(db, [28, 28, 1], batch_size=batch_size)\n",
    "print(provider)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f7ada-d22f-4ad2-8870-81ae776024c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Loading Model (without quantization) ###\")\n",
    "model_sub = n2d2.cells.Sequence([\n",
    "    Conv(1, 6, kernel_dims=[5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [4, 4], **conv_conf()),\n",
    "    Fc(120, 84, **fc_conf1()),\n",
    "    Fc(84, 10, **fc_conf2()),\n",
    "])\n",
    "print(model_sub)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0d1c77-e8e7-46e1-b1a1-bec3e9a216a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_sub.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_sub(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_sub.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_sub(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "        \n",
    "print(\"\\n\")       \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab26754d-1adb-4cc9-9260-585a0162f4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_sub/Conv_9.syntxt\n",
      "Import ./model_sub/Conv_10.syntxt\n",
      "Import ./model_sub/Conv_11.syntxt\n",
      "Import ./model_sub/Fc_6.syntxt\n",
      "Import ./model_sub/Fc_7.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 94.85%\n",
      "Inference time: 0 min 4.351097 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 4.351097 sec\n"
     ]
    }
   ],
   "source": [
    "model_sub.import_free_parameters(\"./model_sub\", ignore_not_exists=True)\n",
    "target = n2d2.target.Score(provider)\n",
    "provider.set_partition('Test')\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_sub.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_sub(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "#target.log_confusion_matrix(\"model_sub\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_sub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45d8e29-9adf-4c4a-a3a8-8b5048a6ddb1",
   "metadata": {},
   "source": [
    "## N-MNIST : Only Positive Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "29d1f655-5dd3-4356-9965-8aff4fce858c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_final/data_positive.pkl', 'rb') as file:\n",
    "    data_positive = pickle.load(file)\n",
    "with open('data_final/labels_positive.pkl', 'rb') as file:\n",
    "    labels_positive = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de80c1b-64e8-4134-8e89-ef9ec05fc0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = n2d2.database.Numpy(random_partitioning=False)\n",
    "\n",
    "db.load(data_positive, labels_positive)\n",
    "db.partition_stimuli(5/7, 1/7, 1/7) # training: 50k, validation: 10k, test: 10k\n",
    "\n",
    "print(\"\\n### Create Provider ###\")\n",
    "batch_size = 64\n",
    "provider = n2d2.provider.DataProvider(db, [28, 28, 1], batch_size=batch_size)\n",
    "print(provider)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4631667d-1bc8-470d-8785-b74e39dab678",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Loading Model (without quantization) ###\")\n",
    "model_positive = n2d2.cells.Sequence([\n",
    "    Conv(1, 6, kernel_dims=[5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [4, 4], **conv_conf()),\n",
    "    Fc(120, 84, **fc_conf1()),\n",
    "    Fc(84, 10, **fc_conf2()),\n",
    "])\n",
    "print(model_positive)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905f13d3-e3ca-4450-964d-cba624b3e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_positive.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_positive(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_positive.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_positive(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "print(\"\\n\")        \n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c128fac5-1388-4912-96e4-7e9eb3541d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_positive/Conv_12.syntxt\n",
      "Import ./model_positive/Conv_13.syntxt\n",
      "Import ./model_positive/Conv_14.syntxt\n",
      "Import ./model_positive/Fc_8.syntxt\n",
      "Import ./model_positive/Fc_9.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 93.21%\n",
      "Inference time: 0 min 4.595262 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 4.595262 sec\n"
     ]
    }
   ],
   "source": [
    "model_positive.import_free_parameters(\"./model_positive\", ignore_not_exists=True)\n",
    "target = n2d2.target.Score(provider)\n",
    "provider.set_partition('Test')\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_positive.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_positive(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "#target.log_confusion_matrix(\"model_positive\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be3c63-f631-4b2d-986d-1f629692d1bb",
   "metadata": {},
   "source": [
    "## N-MNIST : Only Negative Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67d4a76c-dace-445a-82d7-e0937bd62143",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_final/data_negative.pkl', 'rb') as file:\n",
    "    data_negative = pickle.load(file)\n",
    "with open('data_final/labels_negative.pkl', 'rb') as file:\n",
    "    labels_negative = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22de6dc-ef2a-4904-8027-71d9550ea519",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = n2d2.database.Numpy(random_partitioning=False)\n",
    "\n",
    "db.load(data_negative, labels_negative)\n",
    "db.partition_stimuli(5/7, 1/7, 1/7) # training: 50k, validation: 10k, test: 10k\n",
    "\n",
    "print(\"\\n### Create Provider ###\")\n",
    "batch_size = 64\n",
    "provider = n2d2.provider.DataProvider(db, [28, 28, 1], batch_size=batch_size)\n",
    "print(provider)\n",
    "target = n2d2.target.Score(provider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55098c7-9d39-422f-87ad-2f02abe6e3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### Loading Model (without quantization) ###\")\n",
    "model_negative = n2d2.cells.Sequence([\n",
    "    Conv(1, 6, kernel_dims=[5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(6, 16, [5, 5], **conv_conf()),\n",
    "    Pool2d(pool_dims=[2, 2], stride_dims=[2, 2], pooling=\"Average\"),\n",
    "    Conv(16, 120, [4, 4], **conv_conf()),\n",
    "    Fc(120, 84, **fc_conf1()),\n",
    "    Fc(84, 10, **fc_conf2()),\n",
    "])\n",
    "print(model_negative)\n",
    "softmax = n2d2.cells.Softmax(with_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa53729f-1caf-4173-9f5b-c790d00c4c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epochs = 15\n",
    "print(\"\\n### Training ###\")\n",
    "\n",
    "start_training_time = datetime.datetime.now()\n",
    "print(\"Start time Training: \" + str(start_training_time))\n",
    "for epoch in range(nb_epochs):\n",
    "    provider.set_partition(\"Learn\")\n",
    "    model_negative.learn()\n",
    "    print(\"\\n# Train Epoch: \" + str(epoch) + \" #\")\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Learn')/batch_size)):\n",
    "        x = provider.read_random_batch()\n",
    "        x = model_negative(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        x.back_propagate()\n",
    "        x.update()\n",
    "        print(\"Example: \" + str(i * batch_size) + \", loss: \"+ \"{0:.3f}\".format(x[0]), end='\\r')\n",
    "        \n",
    "    print(\"\\n### Validation ###\")\n",
    "    target.clear_success()\n",
    "    provider.set_partition('Validation')\n",
    "    model_negative.test()\n",
    "    for i in range(math.ceil(db.get_nb_stimuli('Validation') / batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_negative(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "        print(\"Test: \" + str(i * batch_size) + \", success: \"+ \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "\n",
    "print(\"\\n\")\n",
    "end_training_time = datetime.datetime.now()\n",
    "print(\"End time Training: \" + str(end_training_time))\n",
    "training_time = end_training_time - start_training_time\n",
    "minutes, seconds = divmod(training_time.total_seconds(), 60)\n",
    "print(\"Training time: \" + str(int(minutes)) + \" min \" + str(int(seconds)) + \" sec \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bfb2aba5-f98b-40be-9891-fd2c8684854a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ./model_negative/Conv_15.syntxt\n",
      "Import ./model_negative/Conv_16.syntxt\n",
      "Import ./model_negative/Conv_17.syntxt\n",
      "Import ./model_negative/Fc_10.syntxt\n",
      "Import ./model_negative/Fc_11.syntxt\n",
      "\n",
      "### Testing ###\n",
      "\n",
      "### Testing - Iteration 1/1 ###\n",
      "\n",
      "Example: 9984, test success: 93.78%\n",
      "Inference time: 0 min 5.329224 sec\n",
      "\n",
      "Average Inference time over 1 iterations: 0 min 5.329224 sec\n"
     ]
    }
   ],
   "source": [
    "model_negative.import_free_parameters(\"./model_negative\", ignore_not_exists=True)\n",
    "target = n2d2.target.Score(provider)\n",
    "provider.set_partition('Test')\n",
    "print(\"\\n### Testing ###\")\n",
    "\n",
    "model_negative.test()\n",
    "\n",
    "num_tests = 1\n",
    "total_inference_time = datetime.timedelta()\n",
    "\n",
    "for test_iteration in range(num_tests):\n",
    "    print(f\"\\n### Testing - Iteration {test_iteration + 1}/{num_tests} ###\\n\")\n",
    "    start_testing_time = datetime.datetime.now()\n",
    "    for i in range(math.ceil(provider.get_database().get_nb_stimuli('Test')/batch_size)):\n",
    "        batch_idx = i*batch_size\n",
    "    \n",
    "        x = provider.read_batch(batch_idx)\n",
    "        x = model_negative(x)\n",
    "        x = softmax(x)\n",
    "        x = target(x)\n",
    "    \n",
    "        print(\"Example: \" + str(i * batch_size) + \", test success: \"\n",
    "              + \"{0:.2f}\".format(100 * target.get_average_success()) + \"%\", end='\\r')\n",
    "    \n",
    "    end_testing_time = datetime.datetime.now()\n",
    "    inference_time = end_testing_time - start_testing_time\n",
    "    total_inference_time += inference_time\n",
    "    minutes, seconds = divmod(inference_time.total_seconds(), 60)\n",
    "    print(\"\\nInference time: \" + str(int(minutes)) + \" min \" + str(seconds) + \" sec\")\n",
    "\n",
    "average_inference_time = total_inference_time / num_tests\n",
    "avg_minutes, avg_seconds = divmod(average_inference_time.total_seconds(), 60)\n",
    "print(\"\\nAverage Inference time over {} iterations: {} min {} sec\".format(num_tests, int(avg_minutes), avg_seconds))\n",
    "\n",
    "# save a confusion matrix\n",
    "#target.log_confusion_matrix(\"model_negative\")\n",
    "# Exporting weights #\n",
    "#x.get_deepnet().export_network_free_parameters(\"./model_negative\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
